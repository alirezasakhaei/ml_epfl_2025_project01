{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e273f47",
   "metadata": {},
   "source": [
    "# 1.Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9befc230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "from implementations import *\n",
    "from medical_dataset_helpers.pipeline import Pipeline\n",
    "from medical_dataset_helpers.preprocessing import MinMaxScaler, StandardScaler\n",
    "from medical_dataset_helpers.linear_models import LogisticRegression\n",
    "from medical_dataset_helpers.metrics_utils import accuracy_score\n",
    "from medical_dataset_helpers.model_selection import train_test_split\n",
    "from medical_dataset_helpers.ensemble_models import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6795a4b7",
   "metadata": {},
   "source": [
    "# 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c1765e",
   "metadata": {},
   "source": [
    "## 2.1 Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b4a311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(\"./data/dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93884e17",
   "metadata": {},
   "source": [
    "## 2.2 Basic Data Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c288a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 321)\n",
      "(109379, 321)\n",
      "(328135,)\n",
      "(328135,)\n",
      "(109379,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(train_ids.shape)\n",
    "print(test_ids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316d0f7",
   "metadata": {},
   "source": [
    "### 2.2.1 NaN analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba931fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaN values in x_train: 47175779\n",
      "Percentage: 44.79%\n",
      "\n",
      "Total NaN values in x_test: 15724379\n",
      "Percentage: 44.79%\n"
     ]
    }
   ],
   "source": [
    "total_nans = np.isnan(x_train).sum()\n",
    "print(f\"Total NaN values in x_train: {total_nans}\")\n",
    "print(f\"Percentage: {100 * total_nans / x_train.size:.2f}%\")\n",
    "\n",
    "total_nans_test = np.isnan(x_test).sum()\n",
    "print(f\"\\nTotal NaN values in x_test: {total_nans_test}\")\n",
    "print(f\"Percentage: {100 * total_nans_test / x_test.size:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6eb17526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns with NaN: 239\n"
     ]
    }
   ],
   "source": [
    "cols_with_nans = np.where(np.isnan(x_train).any(axis=0))[0]\n",
    "print(f\"Number of columns with NaN: {len(cols_with_nans)}\")\n",
    "\n",
    "nans_per_col = np.isnan(x_train).sum(axis=0)\n",
    "cols_with_nans_counts = nans_per_col[cols_with_nans]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c0f642d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplt\u001b[49m.figure(figsize=(\u001b[32m12\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      3\u001b[39m nans_per_col_hist = np.isnan(x_train).sum(axis=\u001b[32m0\u001b[39m)\n\u001b[32m      4\u001b[39m plt.hist(nans_per_col_hist, bins=\u001b[32m50\u001b[39m, edgecolor=\u001b[33m'\u001b[39m\u001b[33mblack\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "nans_per_col_hist = np.isnan(x_train).sum(axis=0)\n",
    "plt.hist(nans_per_col_hist, bins=50, edgecolor='black')\n",
    "plt.xlabel('Number of NaNs per Column')\n",
    "plt.ylabel('Frequency (Number of Columns)')\n",
    "plt.title('Distribution of NaN Counts Across Columns')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae9ea37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NaN Summary Statistics ===\n",
      "\n",
      "Per Column:\n",
      "  Mean NaNs per column: 146965.04\n",
      "  Max NaNs in a column: 328103\n",
      "  Columns with ALL NaNs: 0\n",
      "  Columns with NO NaNs: 82\n"
     ]
    }
   ],
   "source": [
    "print(\"=== NaN Summary Statistics ===\\n\")\n",
    "\n",
    "print(\"Per Column:\")\n",
    "print(f\"  Mean NaNs per column: {np.mean(nans_per_col):.2f}\")\n",
    "print(f\"  Max NaNs in a column: {np.max(nans_per_col)}\")\n",
    "print(f\"  Columns with ALL NaNs: {np.sum(nans_per_col == x_train.shape[0])}\")\n",
    "print(f\"  Columns with NO NaNs: {np.sum(nans_per_col == 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb0c6e7",
   "metadata": {},
   "source": [
    "## 2.3 Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0861dbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns with NaN: 239\n",
      "Number of columns without NaN: 82\n",
      "\n",
      "Original shapes:\n",
      "  x_train: (328135, 321)\n",
      "  x_test: (109379, 321)\n",
      "\n",
      "Cleaned shapes:\n",
      "  x_train_clean: (328135, 82)\n",
      "  x_test_clean: (109379, 82)\n",
      "\n",
      "Removed 239 columns\n",
      "Remaining columns: 82 out of 321\n"
     ]
    }
   ],
   "source": [
    "cols_with_nans = np.where(np.isnan(x_train).any(axis=0))[0]\n",
    "print(f\"Number of columns with NaN: {len(cols_with_nans)}\")\n",
    "temporary_stacked_x = np.vstack((x_train, x_test))\n",
    "valid_cols = np.where(~np.isnan(temporary_stacked_x).any(axis=0))[0]\n",
    "print(f\"Number of columns without NaN: {len(valid_cols)}\")\n",
    "x_train_clean = x_train[:, valid_cols]\n",
    "x_test_clean = x_test[:, valid_cols]\n",
    "\n",
    "print(f\"\\nOriginal shapes:\")\n",
    "print(f\"  x_train: {x_train.shape}\")\n",
    "print(f\"  x_test: {x_test.shape}\")\n",
    "\n",
    "print(f\"\\nCleaned shapes:\")\n",
    "print(f\"  x_train_clean: {x_train_clean.shape}\")\n",
    "print(f\"  x_test_clean: {x_test_clean.shape}\")\n",
    "\n",
    "print(f\"\\nRemoved {x_train.shape[1] - x_train_clean.shape[1]} columns\")\n",
    "print(f\"Remaining columns: {x_train_clean.shape[1]} out of {x_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b9fd0",
   "metadata": {},
   "source": [
    "## 2.4 Fix data imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6273e307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset:\n",
      "  Positive samples: 28975\n",
      " Negative samples: 299160\n",
      "  Ratio: 10.32:1\n",
      "\n",
      "Balanced dataset:\n",
      "  Positive samples: 289750\n",
      "  Negative samples: 299160\n",
      "  Total samples: 588910\n",
      "  Ratio: 1.03:1\n"
     ]
    }
   ],
   "source": [
    "y_train_binary = (y_train + 1) // 2\n",
    "n_positive = np.sum(y_train_binary == 1)\n",
    "n_negative = np.sum(y_train_binary == 0)\n",
    "\n",
    "print(f\"Original dataset:\")\n",
    "print(f\"  Positive samples: {n_positive}\")\n",
    "print(f\" Negative samples: {n_negative}\")\n",
    "print(f\"  Ratio: {n_negative/n_positive:.2f}:1\")\n",
    "\n",
    "positive_indices = np.where(y_train_binary == 1)[0]\n",
    "negative_indices = np.where(y_train_binary == 0)[0]\n",
    "\n",
    "n_oversample = n_negative // n_positive \n",
    "oversampled_positive_indices = np.repeat(positive_indices, n_oversample)\n",
    "balanced_indices = np.concatenate([negative_indices, oversampled_positive_indices])\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(balanced_indices)\n",
    "# Use the CLEANED data (without NaN columns) for balancing\n",
    "x_train_balanced = x_train_clean[balanced_indices]\n",
    "y_train_balanced = y_train[balanced_indices]\n",
    "y_train_binary_balanced = (y_train_balanced + 1) // 2\n",
    "\n",
    "print(f\"\\nBalanced dataset:\")\n",
    "print(f\"  Positive samples: {np.sum(y_train_binary_balanced == 1)}\")\n",
    "print(f\"  Negative samples: {np.sum(y_train_binary_balanced == 0)}\")\n",
    "print(f\"  Total samples: {len(x_train_balanced)}\")\n",
    "print(f\"  Ratio: {np.sum(y_train_binary_balanced == 0)/np.sum(y_train_binary_balanced == 1):.2f}:1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e21f75",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "431a9b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_balanced_to_use, x_validation, y_train_balanced_to_use, y_validation = train_test_split(x_train_balanced, y_train_binary_balanced, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ece0d3",
   "metadata": {},
   "source": [
    "## 3.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1c00f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1/3: {'lr__C': 10}\n",
      "Fitting 2/3: {'lr__C': 100}\n",
      "Fitting 3/3: {'lr__C': 1000}\n",
      "Refitting best estimator with params: {'lr__C': 1000}\n",
      "Best parameters: {'lr__C': 1000}\n",
      "Best CV score: 0.7226125385060405\n",
      "\n",
      "==================================================\n",
      "\n",
      "Classification Report on Validation Set:\n",
      "==================================================\n",
      "Accuracy: 0.725424937597\n",
      "\n",
      "                 precision     recall   f1-score    support\n",
      "\n",
      "              0       0.75       0.68       0.72      59861\n",
      "              1       0.70       0.77       0.73      57921\n",
      "\n",
      "       accuracy                             0.73     117782\n",
      "\n",
      "      macro avg       0.73       0.73       0.73     117782\n",
      "   weighted avg       0.73       0.73       0.72     117782\n"
     ]
    }
   ],
   "source": [
    "from medical_dataset_helpers.model_selection import KFold, GridSearchCV\n",
    "from medical_dataset_helpers.metrics_utils import classification_report\n",
    "from medical_dataset_helpers.linear_models import LogisticRegression\n",
    "from medical_dataset_helpers.preprocessing import MinMaxScaler\n",
    "from medical_dataset_helpers.pipeline import Pipeline\n",
    "\n",
    "kfold = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "lr = LogisticRegression(max_iter=1000, learning_rate=0.1, penalty='l2', random_state=42, C=1000)\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('lr', lr)\n",
    "])\n",
    "\n",
    "param_grid = {'lr__C': [10, 100, 1000]}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring='accuracy',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(x_train_balanced_to_use, y_train_balanced_to_use)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV score:\", grid_search.best_score_)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "y_validation_pred = grid_search.predict(x_validation)\n",
    "\n",
    "print(\"Classification Report on Validation Set:\")\n",
    "print(\"=\"*50)\n",
    "classification_report(y_validation, y_validation_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3baac912",
   "metadata": {},
   "source": [
    "## 3.2 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190df541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from medical_dataset_helpers.neighbors import KNeighborsClassifier\n",
    "from medical_dataset_helpers.model_selection import KFold, GridSearchCV\n",
    "from medical_dataset_helpers.metrics_utils import classification_report\n",
    "\n",
    "n_samples = x_train_balanced_to_use.shape[0]\n",
    "subset_size = n_samples // 3\n",
    "rng = np.random.RandomState(42)\n",
    "subset_idx = rng.permutation(n_samples)[:subset_size]\n",
    "\n",
    "X_sub = x_train_balanced_to_use[subset_idx]\n",
    "y_sub = y_train_balanced_to_use[subset_idx]\n",
    "\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 5, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "}\n",
    "\n",
    "grid_search_knn = GridSearchCV(\n",
    "    estimator=knn,\n",
    "    param_grid=param_grid,\n",
    "    cv=kfold,\n",
    "    scoring='accuracy',\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "grid_search_knn.fit(X_sub, y_sub)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_knn.best_params_)\n",
    "print(\"Best CV score:\", grid_search_knn.best_score_)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "y_val_pred_knn = grid_search_knn.predict(x_validation)\n",
    "\n",
    "print(\"KNN Classification Report on Validation Set:\")\n",
    "print(\"=\"*50)\n",
    "classification_report(y_validation, y_val_pred_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2c1f80",
   "metadata": {},
   "source": [
    "## 3.3 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee89a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from medical_dataset_helpers.ensemble_models import RandomForestClassifier\n",
    "from medical_dataset_helpers.metrics_utils import classification_report\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=300,\n",
    "    criterion='gini',\n",
    "    max_features='sqrt',\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # use all cores\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "rf.fit(x_train_balanced_to_use, y_train_balanced_to_use)\n",
    "\n",
    "y_val_pred_rf = rf.predict(x_validation)\n",
    "\n",
    "print(\"Random Forest (max_depth=300) Classification Report on Validation Set:\")\n",
    "print(\"=\"*50)\n",
    "classification_report(y_validation, y_val_pred_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be372ea",
   "metadata": {},
   "source": [
    "## 3.4 MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de646290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Classification Report on Validation Set:\n",
      "==================================================\n",
      "Accuracy: 0.795673362653\n",
      "\n",
      "                 precision     recall   f1-score    support\n",
      "\n",
      "              0       0.82       0.76       0.79      59861\n",
      "              1       0.77       0.83       0.80      57921\n",
      "\n",
      "       accuracy                             0.80     117782\n",
      "\n",
      "      macro avg       0.80       0.80       0.80     117782\n",
      "   weighted avg       0.80       0.80       0.80     117782\n"
     ]
    }
   ],
   "source": [
    "from medical_dataset_helpers.neural_network import MLPClassifier\n",
    "from medical_dataset_helpers.metrics_utils import classification_report\n",
    "from medical_dataset_helpers.preprocessing import MinMaxScaler\n",
    "from medical_dataset_helpers.pipeline import Pipeline\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate_init=1e-3,\n",
    "    alpha=1e-4,\n",
    "    max_iter=100,\n",
    "    batch_size='auto',\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    tol=1e-4,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('mlp', mlp)\n",
    "])\n",
    "\n",
    "pipeline.fit(x_train_balanced_to_use, y_train_balanced_to_use)\n",
    "y_val_pred_mlp = pipeline.predict(x_validation)\n",
    "\n",
    "print(\"MLP Classification Report on Validation Set:\")\n",
    "print(\"=\"*50)\n",
    "classification_report(y_validation, y_val_pred_mlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r-reticulate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
